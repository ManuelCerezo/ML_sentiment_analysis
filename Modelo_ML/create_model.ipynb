{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/03 11:41:49 WARN Utils: Your hostname, herex resolves to a loopback address: 127.0.1.1; using 10.1.201.237 instead (on interface wlp58s0)\n","22/05/03 11:41:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","WARNING: An illegal reflective access operation has occurred\n","WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/herex/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n","WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n","WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n","WARNING: All illegal access operations will be denied in a future release\n","Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/05/03 11:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["import findspark\n","findspark.init()\n","import pyspark\n","sc = pyspark.SparkContext(appName=\"LOGISTICREG\")\n","from pyspark.sql.session import SparkSession\n","spark = SparkSession(sc)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#read the dataset\n","df=spark.read.csv('data.csv',inferSchema=True,header=True)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: integer (nullable = true)\n"," |-- date: string (nullable = true)\n"," |-- news: string (nullable = true)\n"," |-- final_manual_labelling: integer (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+--------------------+----------------------+\n","| id|     date|                news|final_manual_labelling|\n","+---+---------+--------------------+----------------------+\n","|  0|1/25/2022|Ripple announces ...|                     1|\n","|  1|1/25/2022|IMF directors urg...|                    -1|\n","|  2|1/25/2022|Dragonfly Capital...|                     1|\n","|  3|1/25/2022|Rick and Morty co...|                     0|\n","|  4|1/25/2022|How fintech SPACs...|                     0|\n","+---+---------+--------------------+----------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.show(5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+---------------------------------------------------------------------------------------------------+\n","|id |date     |tokens                                                                                             |\n","+---+---------+---------------------------------------------------------------------------------------------------+\n","|0  |1/25/2022|[ripple, announces, stock, buyback,, nabs, $15, billion, valuation]                                |\n","|1  |1/25/2022|[imf, directors, urge, el, salvador, to, remove, bitcoin, as, legal, tender]                       |\n","|2  |1/25/2022|[dragonfly, capital, is, raising, $500, million, for, new, fund]                                   |\n","|3  |1/25/2022|[rick, and, morty, co-creator, collaborates, with, paradigm, on, nft, research, project]           |\n","|4  |1/25/2022|[how, fintech, spacs, lost, their, shine]                                                          |\n","|5  |1/25/2022|[multichain, vulnerability, put, a, billion, dollars, at, risk,, says, firm, that, found, the, bug]|\n","|6  |1/25/2022|[youtube, wants, to, help, content, creators, capitalize, on, nfts]                                |\n","|7  |1/25/2022|[opensea, is, reimbursing, users, who, sold, nfts, below, market, value, due, to, ui, issue]       |\n","|8  |1/25/2022|[gooddollar, launches, key, protocol, upgrade, to, expand, crypto-backed, ubi, ecosystem]          |\n","|9  |1/25/2022|[bcb, group, raises, a, $60, million, series, a, round, co-led, by, foundation, capital]           |\n","+---+---------+---------------------------------------------------------------------------------------------------+\n","only showing top 10 rows\n","\n"]}],"source":["from pyspark.ml.feature import Tokenizer\n","tokenization=Tokenizer(inputCol='news',outputCol='tokens')\n","tokenized_df=tokenization.transform(df)\n","tokenized_df.select(['id','date','tokens']).show(10,False)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+-------------------------------------------------------------------------------------+\n","|id |date     |refined_tokens                                                                       |\n","+---+---------+-------------------------------------------------------------------------------------+\n","|0  |1/25/2022|[ripple, announces, stock, buyback,, nabs, $15, billion, valuation]                  |\n","|1  |1/25/2022|[imf, directors, urge, el, salvador, remove, bitcoin, legal, tender]                 |\n","|2  |1/25/2022|[dragonfly, capital, raising, $500, million, new, fund]                              |\n","|3  |1/25/2022|[rick, morty, co-creator, collaborates, paradigm, nft, research, project]            |\n","|4  |1/25/2022|[fintech, spacs, lost, shine]                                                        |\n","|5  |1/25/2022|[multichain, vulnerability, put, billion, dollars, risk,, says, firm, found, bug]    |\n","|6  |1/25/2022|[youtube, wants, help, content, creators, capitalize, nfts]                          |\n","|7  |1/25/2022|[opensea, reimbursing, users, sold, nfts, market, value, due, ui, issue]             |\n","|8  |1/25/2022|[gooddollar, launches, key, protocol, upgrade, expand, crypto-backed, ubi, ecosystem]|\n","|9  |1/25/2022|[bcb, group, raises, $60, million, series, round, co-led, foundation, capital]       |\n","+---+---------+-------------------------------------------------------------------------------------+\n","only showing top 10 rows\n","\n"]}],"source":["from pyspark.ml.feature import StopWordsRemover\n","stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n","refined_df=stopword_removal.transform(tokenized_df)\n","refined_df.select(['id','date','refined_tokens']).show(10,False)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n","|id |refined_tokens                                                           |features                                                                            |\n","+---+-------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n","|0  |[ripple, announces, stock, buyback,, nabs, $15, billion, valuation]      |(6332,[13,30,64,131,398,429,962,5255],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n","|1  |[imf, directors, urge, el, salvador, remove, bitcoin, legal, tender]     |(6332,[2,122,243,339,612,1205,1309,2930,5019],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n","|2  |[dragonfly, capital, raising, $500, million, new, fund]                  |(6332,[1,3,19,42,470,559,1522],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n","|3  |[rick, morty, co-creator, collaborates, paradigm, nft, research, project]|(6332,[4,52,402,421,3624,3821,4271,5010],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n","+---+-------------------------------------------------------------------------+------------------------------------------------------------------------------------+\n","only showing top 4 rows\n","\n","<class 'pyspark.sql.dataframe.DataFrame'>\n"]}],"source":["# Count Vectorizer\n","from pyspark.ml.feature import CountVectorizer\n","count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n","cv_df=count_vec.fit(refined_df).transform(refined_df)\n","cv_df.select(['id','refined_tokens','features']).show(4,False)\n","\n","\n","print(type(cv_df))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------+-----+\n","|final_manual_labelling|count|\n","+----------------------+-----+\n","|                    -1|  258|\n","|                     1| 1184|\n","|                     0| 1241|\n","+----------------------+-----+\n","\n"]}],"source":["text_df=cv_df.filter(((cv_df.final_manual_labelling =='1') | (cv_df.final_manual_labelling =='-1')| (cv_df.final_manual_labelling =='0')))\n","text_df.groupBy('final_manual_labelling').count().show()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+-----------+\n","|                news|              tokens|      refined_tokens|            features|token_count|\n","+--------------------+--------------------+--------------------+--------------------+-----------+\n","|Visa announces cr...|[visa, announces,...|[visa, announces,...|(9,[0,1,2,3,4,5,6...|          9|\n","+--------------------+--------------------+--------------------+--------------------+-----------+\n","\n"]}],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.functions import *\n","\n","len_udf = udf(lambda s: len(s), IntegerType())\n","\n","refined_text_df = cv_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n","refined_text_df.orderBy(rand()).show(10)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["refined_text_df = refined_text_df.withColumn(\"Label\", refined_text_df.final_manual_labelling.cast('float')).drop('final_manual_labelling')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["['id',\n"," 'date',\n"," 'news',\n"," 'tokens',\n"," 'refined_tokens',\n"," 'features',\n"," 'token_count',\n"," 'Label']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["refined_text_df.columns"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------+-----------+-----+\n","|features                                                                            |token_count|Label|\n","+------------------------------------------------------------------------------------+-----------+-----+\n","|(6332,[0,650,832,964,978,3215,3220,3310,3603],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|9          |0.0  |\n","|(6332,[8,253,341,589],[1.0,1.0,1.0,1.0])                                            |4          |0.0  |\n","|(6332,[8,48,141,197,1007,2759,3425,5585],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |8          |1.0  |\n","|(6332,[112,149,153,192,211,689,893,983],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])          |8          |0.0  |\n","|(6332,[4,12,27,30,203,2494,4590],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |7          |0.0  |\n","|(6332,[0,9,102,535,645,884,2151,6200],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |8          |-1.0 |\n","|(6332,[0,19,22,28,67,402,869,1998,5388],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])      |9          |0.0  |\n","|(6332,[21,100,265,364,391,687,4282,4810],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |8          |1.0  |\n","|(6332,[0,76,78,87,1116,1214,2045,2853],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])           |8          |1.0  |\n","|(6332,[0,3,233,644,1015,3559,4542],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |7          |0.0  |\n","+------------------------------------------------------------------------------------+-----------+-----+\n","only showing top 10 rows\n","\n"]}],"source":["refined_text_df.orderBy(rand()).select(['features','token_count','Label']).show(10,False)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+\n","| id|     date|                news|              tokens|      refined_tokens|            features|token_count|Label|LabelNegative|LabelPositive|\n","+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+\n","|  0|1/25/2022|Ripple announces ...|[ripple, announce...|[ripple, announce...|(6332,[13,30,64,1...|          8|  1.0|          0.0|          1.0|\n","|  1|1/25/2022|IMF directors urg...|[imf, directors, ...|[imf, directors, ...|(6332,[2,122,243,...|          9| -1.0|          1.0|          0.0|\n","|  2|1/25/2022|Dragonfly Capital...|[dragonfly, capit...|[dragonfly, capit...|(6332,[1,3,19,42,...|          7|  1.0|          0.0|          1.0|\n","|  3|1/25/2022|Rick and Morty co...|[rick, and, morty...|[rick, morty, co-...|(6332,[4,52,402,4...|          8|  0.0|          0.0|          0.0|\n","|  4|1/25/2022|How fintech SPACs...|[how, fintech, sp...|[fintech, spacs, ...|(6332,[149,1433,4...|          4|  0.0|          0.0|          0.0|\n","+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+\n","only showing top 5 rows\n","\n"]}],"source":["#Tenemos que crear dos modelos ,valor positivo y el resto, y valor negativo y el resto (1,0), y comparar los dos modelos (-1,0)\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import *\n","\n","funct_negative_label = udf(lambda x: 1.00 if x == -1 else 0.00, FloatType())\n","func_positive_label = udf(lambda x: 1.00 if x == 1 else 0.00, FloatType())\n","\n","refined_text_df = refined_text_df.withColumn(\"LabelNegative\",funct_negative_label('Label'))\n","refined_text_df = refined_text_df.withColumn(\"LabelPositive\",func_positive_label('Label'))\n","\n","refined_text_df.show(5)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Veamos si las dos matrices generadas cumplen las caracteristicas que queriamos"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#Ponemos en el filtro todos los casos 1,0,-1 para ver que las negativas no tenga ningun 1, y las positivas ningun -1\n","text_df_positivo=refined_text_df.filter(((refined_text_df.LabelPositive =='1') | (refined_text_df.LabelPositive =='-1')| (refined_text_df.LabelPositive =='0')))\n","text_df_negativo=refined_text_df.filter(((refined_text_df.LabelNegative =='1') | (refined_text_df.LabelNegative =='-1')| (refined_text_df.LabelNegative =='0')))\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+-----+\n","|LabelNegative|count|\n","+-------------+-----+\n","|          1.0|  258|\n","|          0.0| 2425|\n","+-------------+-----+\n","\n"]}],"source":["text_df_negativo.groupBy('LabelNegative').count().show()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+-----+\n","|LabelPositive|count|\n","+-------------+-----+\n","|          1.0| 1184|\n","|          0.0| 1499|\n","+-------------+-----+\n","\n"]}],"source":["text_df_positivo.groupBy('LabelPositive').count().show()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","\n","df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\n","model_text_df = df_assembler.transform(refined_text_df)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|        features_vec|\n","+--------------------+\n","|(6333,[13,30,64,1...|\n","+--------------------+\n","only showing top 1 row\n","\n"]}],"source":["model_text_df.select(['features_vec']).show(1)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n","#split the data \n","training_df_negative,test_df_negative=model_text_df.randomSplit([0.75,0.25])\n","#split the data \n","training_df_positive,test_df_positive=model_text_df.randomSplit([0.75,0.25])"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/03 11:42:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","22/05/03 11:42:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"]}],"source":["log_reg_positive = LogisticRegression(featuresCol='features_vec',labelCol='LabelPositive').fit(training_df_positive)\n","log_reg_negative = LogisticRegression(featuresCol='features_vec',labelCol='LabelNegative').fit(training_df_negative)\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# log_reg_negative.write().save(\"./model_neg\")\n","# log_reg_positive.write().save(\"./model_pos\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["model_neg = LogisticRegressionModel.load(\"model_neg\")\n","model_pos = LogisticRegressionModel.load(\"model_pos\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- news: string (nullable = true)\n","\n","+-----------------------------------------------------------------------------------------------------+\n","|tokens                                                                                               |\n","+-----------------------------------------------------------------------------------------------------+\n","|[visa, announces, crypto, partnership, with, neobank, focused, on, services, for, black, communities]|\n","+-----------------------------------------------------------------------------------------------------+\n","\n","+--------------------------------------------------------------------------------------+\n","|refined_tokens                                                                        |\n","+--------------------------------------------------------------------------------------+\n","|[visa, announces, crypto, partnership, neobank, focused, services, black, communities]|\n","+--------------------------------------------------------------------------------------+\n","\n","+--------------------+--------------------+\n","|            features|      refined_tokens|\n","+--------------------+--------------------+\n","|(9,[0,1,2,3,4,5,6...|[visa, announces,...|\n","+--------------------+--------------------+\n","\n"]},{"data":{"text/plain":["(DenseVector([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 9.0]),\n"," SparseVector(6333, {4: 1.0, 52: 1.0, 402: 1.0, 421: 1.0, 3624: 1.0, 3821: 1.0, 4271: 1.0, 5010: 1.0, 6332: 8.0}))"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.ml.linalg import Vectors\n","\n","\n","df = spark.createDataFrame([(\"Visa announces crypto partnership with neobank focused on services for Black communities\",)], [\"news\"])\n","\n","df.printSchema()\n","\n","\n","tokenization=Tokenizer(inputCol='news',outputCol='tokens')\n","tokenized_df=tokenization.transform(df)\n","tokenized_df.select(['tokens']).show(10,False)\n","\n","stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n","refined_df=stopword_removal.transform(tokenized_df)\n","refined_df.select(['refined_tokens']).show(10,False)\n","\n","count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n","cv_df=count_vec.fit(refined_df).transform(refined_df)\n","\n","\n","len_udf = udf(lambda s: len(s), IntegerType())\n","\n","refined_text_df = cv_df.withColumn(\"token_count\", len_udf(col('refined_tokens')))\n","\n","refined_text_df.select(['features','refined_tokens']).show(2)\n","\n","\n","df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\n","df = df_assembler.transform(refined_text_df)\n","\n","#df.show(2)\n","\n","df.head().features_vec, test_df_positive.head().features_vec\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["LogisticRegressionModel: uid=LogisticRegression_ff77d2a1c89d, numClasses=2, numFeatures=6333"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model_neg"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/herex/.local/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["results_positive = log_reg_positive.evaluate(test_df_positive).predictions\n","results_negative = log_reg_negative.evaluate(test_df_negative).predictions"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+--------------------+--------------------+--------------------+----------+\n","| id|     date|                news|              tokens|      refined_tokens|            features|token_count|Label|LabelNegative|LabelPositive|        features_vec|       rawPrediction|         probability|prediction|\n","+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+--------------------+--------------------+--------------------+----------+\n","|  3|1/25/2022|Rick and Morty co...|[rick, and, morty...|[rick, morty, co-...|(6332,[4,52,402,4...|          8|  0.0|          0.0|          0.0|(6333,[4,52,402,4...|[3.38166162262683...|[0.96712647395689...|       0.0|\n","|  4|1/25/2022|How fintech SPACs...|[how, fintech, sp...|[fintech, spacs, ...|(6332,[149,1433,4...|          4|  0.0|          0.0|          0.0|(6333,[149,1433,4...|[14.5824306769750...|[0.99999953555892...|       0.0|\n","|  7|1/25/2022|OpenSea is reimbu...|[opensea, is, rei...|[opensea, reimbur...|(6332,[32,38,110,...|         10|  0.0|          0.0|          0.0|(6333,[32,38,110,...|[-1.1446126842184...|[0.24147447353902...|       1.0|\n","| 11|1/25/2022|Twitter is growin...|[twitter, is, gro...|[twitter, growing...|(6332,[0,125,232,...|          5|  1.0|          0.0|          1.0|(6333,[0,125,232,...|[18.8085635897540...|[0.99999999321508...|       0.0|\n","| 26|1/22/2022|El Salvador purch...|[el, salvador, pu...|[el, salvador, pu...|(6332,[1,2,32,53,...|          9|  1.0|          0.0|          1.0|(6333,[1,2,32,53,...|[-18.929177392404...|[6.01399010073269...|       1.0|\n","+---+---------+--------------------+--------------------+--------------------+--------------------+-----------+-----+-------------+-------------+--------------------+--------------------+--------------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["results_positive.show(5)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["#confusion matrix positive model\n","true_postives = results_positive[(results_positive.Label == 1) & (results_positive.prediction == 1)].count()\n","true_negatives = results_positive[(results_positive.Label == 0) & (results_positive.prediction == 0)].count()\n","false_positives = results_positive[(results_positive.Label == 0) & (results_positive.prediction == 1)].count()\n","false_negatives = results_positive[(results_positive.Label == 1) & (results_positive.prediction == 0)].count()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6484641638225256\n"]}],"source":["recall = float(true_postives)/(true_postives + false_negatives)\n","print(recall)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6785714285714286\n"]}],"source":["precision = float(true_postives) / (true_postives + false_positives)\n","print(precision)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6134969325153374\n"]}],"source":["accuracy=float((true_postives+true_negatives) /(results_positive.count()))\n","print(accuracy)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["#confusion matrix negative model\n","true_postives = results_negative[(results_negative.Label == 1) & (results_negative.prediction == 1)].count()\n","true_negatives = results_negative[(results_negative.Label == 0) & (results_negative.prediction == 0)].count()\n","false_positives = results_negative[(results_negative.Label == 0) & (results_negative.prediction == 1)].count()\n","false_negatives = results_negative[(results_negative.Label == 1) & (results_negative.prediction == 0)].count()"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.013888888888888888\n"]}],"source":["recall = float(true_postives)/(true_postives + false_negatives)\n","print(recall)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.16666666666666666\n"]}],"source":["precision = float(true_postives) / (true_postives + false_positives)\n","print(precision)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4386503067484663\n"]}],"source":["accuracy=float((true_postives+true_negatives) /(results_positive.count()))\n","print(accuracy)"]}],"metadata":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
